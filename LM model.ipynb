{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Import Dependencies\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import google.generativeai as genai\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Load Pre-trained Language Model\n",
    "def load_model(model_name=\"gpt2\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Define Inference Function\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " Once upon a time, in a distant land, the sun was shining, and the moon was shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and the stars were shining, and\n"
     ]
    }
   ],
   "source": [
    "### Step 4: Test Model Output\n",
    "prompt = \"Once upon a time, in a distant land\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(\"Generated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Output:\n",
      " Once upon a time, in a distant land shrouded in whispering forests and kissed by the silver light of twin moons, lived a young woman named Elara.  She wasn't a princess, nor a sorceress, but a humble weaver, her days filled with the rhythmic click-clack of her loom and the soft rustle of silk threads.  Her world was small, bounded by the ancient trees that encircled her village and the sparkling river that flowed through its heart.  But Elara possessed a spirit as vast and untamed as the wilderness beyond, and she yearned for something more than the familiar comfort of her home.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def setup_gemini(api_key, model_name=\"gemini-1.5-pro-latest\"):\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    return model\n",
    "\n",
    "def generate_gemini_text(model, prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "api_key = \"AIzaSyApvjwVTxEGd9uSgDVf-fIGBSvtuQL625Y\"\n",
    "gemini_model = setup_gemini(api_key, \"gemini-1.5-pro-latest\")\n",
    "gemini_response = generate_gemini_text(gemini_model, prompt)\n",
    "print(\"Gemini Output:\\n\", gemini_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conclusion: This project demonstrates the implementation and analysis of LMs, comparing GPT-2 and Gemini. It includes performance metrics, bias detection, and research questions to evaluate the effectiveness of the models.\n"
     ]
    }
   ],
   "source": [
    "### Step 10: Conclusion\n",
    "print(\"\\nConclusion: This project demonstrates the implementation and analysis of LMs, comparing GPT-2 and Gemini. It includes performance metrics, bias detection, and research questions to evaluate the effectiveness of the models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
